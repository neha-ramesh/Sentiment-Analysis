# ğŸ™ï¸ Speech Emotion Analyzer

A deep learning-based system for recognizing human emotions from speech ğŸ§.  
This project uses **LSTM** and **Variational Autoencoder (VAE)** models to classify emotions such as:

> ğŸ˜„ Happy &nbsp;&nbsp; ğŸ˜¢ Sad &nbsp;&nbsp; ğŸ˜  Angry &nbsp;&nbsp; ğŸ˜¨ Fearful &nbsp;&nbsp; ğŸ˜² Surprise &nbsp;&nbsp; ğŸ¤¢ Disgust

---

## ğŸ“‚ Datasets Used

We use four well-known speech emotion datasets:

| Dataset      | Samples | Emotions Included                                             | Gender    |
|--------------|---------|--------------------------------------------------------------|-----------|
| **RAVDESS**  | 2452    | Happy, Sad, Angry, Fearful, Surprise, Disgust                | Male, Female |
| **SAVEE**    | 500     | Happy, Sad, Angry, Fearful, Surprise, Disgust                | Male      |
| **TESS**     | 2800    | Happy, Sad, Angry, Fearful, Surprise, Disgust, Neutral       | Female    |
| **CREMA-D**  | 7442    | Happy, Sad, Angry, Fearful, Disgust, Neutral                 | Male, Female |
---

## ğŸ¯ Features Extracted

From each audio sample, we extract meaningful features:

- ğŸ¼ **MFCC** â€“ Captures timbre and tone
- ğŸ¹ **Chroma** â€“ Pitch-based variations
- ğŸ” **ZCR** â€“ Measures voice noisiness
- âš¡ **TEO** â€“ Detects energy bursts
- ğŸ“ˆ **Pitch** â€“ Indicates tone height
- ğŸ”Š **Energy** â€“ Measures loudness

> All features are normalized and stored in a dataframe for model input.
---

## ğŸ§  Model Architectures

### ğŸ” LSTM

An LSTM model is used to capture **temporal patterns** in speech:

- `128` LSTM units â†’ Dropout â†’ Dense layer
- Softmax activation for multiclass emotion classification
- Trained on extracted features to learn emotion sequences over time

### ğŸŒŒ VAE + Classifier

A **Variational Autoencoder (VAE)** reduces dimensionality and learns compressed emotion representations:

- Latent vectors from the VAE encoder are passed into an MLP classifier
- Efficient at emotion classification with fewer parameters
- Helps handle noise and redundancy in features

---

## ğŸ“Š Model Evaluation

### ğŸ“ˆ LSTM Model
- âœ… **Training Accuracy:** `88.75%`
- âœ… **Validation Accuracy:** `91.00%`
- ğŸ“‰ **Loss:** Steady decrease â†’ Proper learning
- âœ… **Confusion Matrix:** Strong on `happy`, `sad`, `angry`
- ğŸ“‹ **Classification Report:**
  - Precision, Recall, F1-Score: **> 85%**
  - Weighted F1 Score: **~91%**

### ğŸŒŒ VAE + Classifier
- âœ… **Training Accuracy:** `82.50%`
- âœ… **Validation Accuracy:** `84.75%`
- ğŸŒ Performs well on `neutral` and `sad`
- âš ï¸ Slight confusion between `angry` and `disgust`

---

## ğŸš€ Technologies Used
- Python, NumPy, Pandas, Librosa
- TensorFlow / Keras
- Matplotlib & Seaborn (for visualizations)
---
